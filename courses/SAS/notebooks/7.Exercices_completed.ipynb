{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql.context import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "sc = pyspark.SparkContext('local[*]') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicios propuestos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio 1**: Escribir una UDF que convierta en minúsculas excepto el primer caracter de la columna *des_nomco* del dataframe *ttgoficiDF* cargado a continuación: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sqlContext = SQLContext(sc)\n",
    "dataPath = \"../data/ttgofici.csv\"\n",
    "ttgoficiDF = sqlContext.read.format(\"com.databricks.spark.csv\")\\\n",
    "            .option(\"header\", \"true\")\\\n",
    "            .load(dataPath) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ttgoficiDF.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# x.capitalize()\n",
    "def lower(string):\n",
    "    return string[0] + string[1:].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lower2(string):\n",
    "    return string.capitalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col,udf\n",
    "udLower = udf(lower, StringType())\n",
    "ttgoficiDF.withColumn(\"des_nomab_lower\",udLower(\"des_nomab\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio 2**: Desarrollar una \"UDF\" que convierta el primer caracter de un string según la siguiente regla:    \n",
    "'1'-> 'a'  \n",
    "'2'-> '1'  \n",
    "'3'-> '2'  \n",
    "'4'-> '3'  \n",
    "'5'-> '4'  \n",
    "'6'-> '5'  \n",
    "'7'-> '6'  \n",
    "'8'-> '7'  \n",
    "De tal forma un String con valor: \"1.Semana del 1 al 7 de agosto\" se convertirá a : \"a..Semana del 1 al 7 de   agosto\", \"2.Semana del 8 al 14 de agosto\" se convertirá a \"1.Semana del 8 al 14 de agosto\"... y así sucesivamente.  \n",
    "Tip: utilizar el siguiente mapping:  \n",
    "mapping = {'1': 'a', '2': '1', '3': '2', '4': '3', '5': '4', '6': '5', '7': '6', '8':'7'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = [('1', '1.Semana del 1 al 7 de agosto'),('2', '2.Semana del 8 al 14 de agosto'),\\\n",
    "     ('3', '3.Semana del 15 al 21 de agosto'),('4', '3.Semana del 15 al 21 de agosto')]\n",
    "DFSpentData =  sqlContext.createDataFrame(data, ['id','Semana'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DFSpentData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mapping = {'1': 'a', '2': '1', '3': '2', '4': '3', '5': '4', '6': '5', '7': '6', '8':'7'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import UserDefinedFunction\n",
    "udfSemana = UserDefinedFunction(lambda x: mapping[x[0]] + x[1:], StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DFSpentData.withColumn(\"semana2\",udfSemana(\"semana\")).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In sql sentence..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sqlContext.registerDataFrameAsTable(DFSpentData,\"SpentData\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sqlContext.registerFunction(\"getDay\", lambda x: mapping[x[0]] + x[1:], StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sqlContext.sql(\"select id, udfSemana(Semana) as semana3 from SpentData\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio 3**: Migrar las siguientes transformaciones en lenguaje a SAS a SparkSQL:\n",
    "\n",
    "data inframul_diaria;  \n",
    "\tset diaria.Nuevanet_di_con;  \n",
    "\n",
    "\tdia = substr(FEC_ULTACCSO,9,2);  \n",
    "\tmes = substr(FEC_ULTACCSO,6,2);  \n",
    "\tanio= substr(FEC_ULTACCSO,1,4);  \n",
    "\tfecha = compress(anio||mes||dia);  \n",
    "\tformat fecha_ult_conexion_semanal date9.;  \n",
    "\tfecha_ult_conexion_semanal =input(fecha, yymmdd10.);  \n",
    "\tcod_persona = input (COD_PERSCTPN, 11.);  \n",
    "run;\n",
    "\n",
    "proc sql;  \n",
    "\tcreate table cli_net_semanal as  \n",
    "\tselect distinct cod_persona,  \n",
    "\t\t\tmax (fecha_ult_conexion_semanal) as fecha_ult_conexion_semanal format date9.  \n",
    "\tfrom inframul_diaria  \n",
    "\twhere fecha_ult_conexion_semanal>=&FEC_INI02  \n",
    "\tgroup by cod_persona \n",
    "\torder by cod_persona;  \n",
    "quit;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query = \"Select *, cod_persctpn as cod_persona FROM nuevanet_di_con\"\n",
    "udf_dia = UserDefinedFunction(lambda x: x[8:10], StringType())\n",
    "udf_mes = UserDefinedFunction(lambda x: x[5:7], StringType())\n",
    "udf_anio = UserDefinedFunction(lambda x: x[0:4], StringType())\n",
    "udf_fecha = UserDefinedFunction(lambda x,y,z: x+y+z, StringType())\n",
    "udf_fecha_mes = UserDefinedFunction(lambda x,y,z: x+mes_l[y]+z, StringType())\n",
    "campo_fecha = 'fec_ultaccso'\n",
    "\n",
    "sqlContext.sql(query).withColumn(\"dia\",udf_dia(campo_fecha))\\\n",
    ".withColumn(\"mes\",udf_mes(campo_fecha))\\\n",
    ".withColumn(\"anio\",udf_anio(campo_fecha))\\\n",
    ".withColumn(\"fecha\",udf_fecha(\"dia\",\"mes\",\"anio\"))\\\n",
    ".withColumn(\"fecha_ult_conexion_semanal\",udf_fecha_mes(\"dia\",\"mes\",\"anio\"))\\\n",
    ".registerTempTable(\"inframul_diaria\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FEC_INI02 = \"01ENE2000\"\n",
    "sqlContext.sql(\"Select distinct cod_persona, max(fecha_ult_conexion_semanal) as fecha_ult_conexion_semanal \\\n",
    "FROM inframul_diaria \\\n",
    "WHERE  fecha_ult_conexion_semanal>='\" + FEC_INI02 + \"'\\\n",
    "GROUP BY cod_persona\")\\\n",
    ".registerTempTable(\"cli_net_semanal\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
