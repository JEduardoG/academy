{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregate functions, such as SUM or MAX, operate on a group of rows and calculate a single return value for every group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that we have a amount spent table and we want to answer the next question:\n",
    "\n",
    "QUESTION 1. What are the biggest amount spent in every mode per person?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql.context import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "sc = pyspark.SparkContext('local[*]') \n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l = [('Bob', '2016-12-27','CreditCard','1',356.80),('Peter', '2016-12-27', 'CreditCard','3',97.0),\\\n",
    "     ('Peter', '2016-12-17', 'Transfer','0',100.0),('Alice', '2016-11-13', 'CreditCard','2',123.50),\\\n",
    "     ('Alice', '2016-12-29', 'CreditCard','3',23.3),('Peter',  '2016-12-20','Transfer','0',900.0), \\\n",
    "     ('Bob', '2016-12-31', 'CreditCard','3',599.50),('Peter', '2016-12-30', 'Withdraw','3',100.0),\\\n",
    "     ('Bob', '2016-12-30', 'Withdraw','5',300.0)]\n",
    "DFSpentData =  sqlContext.createDataFrame(l, ['Name','Date','Mode','Category','AmountSpent'])\n",
    "DFSpentData.registerTempTable(\"SpentData\")\n",
    "DFSpentData.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sqlContext.sql(\"SELECT Name, Mode, max(AmountSpent) as maxAmount FROM SpentData GROUP BY Name, Mode\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QUESTION 2. Who is the person more spender in every mode?\n",
    "Let's see with window functions..\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Window Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A window function calculates a return value for every input row of a table based on a group of rows, called the Frame.  \n",
    "Every input row can have a unique frame associated with it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use window functions, users need to mark that a function is used as a window function by either  \n",
    "\n",
    "-Adding an OVER clause after a supported function in SQL, e.g. avg(value) OVER (...); or  \n",
    "-Calling the over method on a supported function in the DataFrame API, e.g. rank().over(...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sqlContext.sql(\"SELECT Name, Mode, max(AmountSpent) OVER (PARTITION BY Name, Mode) as maxAmount FROM SpentData\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Window Spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once a function is marked as a window function, the next key step is to define the Window Specification associated   with this function. A window specification defines which rows are included in the frame associated with a given   input row. A window specification includes three parts:  \n",
    "\n",
    "  * 1.Partitioning Specification: controls which rows will be in the same partition with the given row. Also,  \n",
    "the user might want to make sure all rows having the same value for  the category column are collected to the  \n",
    "same machine before ordering and calculating the frame. If no partitioning specification is given, then all data must be collected to a single machine.  \n",
    "  * 2.Ordering Specification: controls the way that rows in a partition are ordered, determining the position of the   given row in its partition.  \n",
    "  * 3.Frame Specification: states which rows will be included in the frame for the current input row, based on their   relative position to the current row.  For example, “the three rows preceding the current row to the current row”   describes a frame including the current input row and three rows appearing before the current row."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The grammar of windows operators in SQL accepts the following:  OVER (PARTITION BY ... ORDER BY ...)  \n",
    "  * CLUSTER BY or PARTITION BY or DISTRIBUTE BY for partitions,  \n",
    "  * ORDER BY or SORT BY for sorting order,  \n",
    "  * RANGE, ROWS, RANGE BETWEEN, and ROWS BETWEEN for window frame types,  \n",
    "  * UNBOUNDED PRECEDING, UNBOUNDED FOLLOWING, CURRENT ROW for frame bounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sqlContext.sql(\"SELECT Name, Date, Mode, AmountSpent,\\\n",
    "rank() OVER (PARTITION BY Name ORDER BY AmountSpent DESC) as rankAmount FROM SpentData\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### USING DATAFRAME API "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the DataFrame API, we provide utility functions to define a window specification.  \n",
    "Taking Python as an example, users can specify partitioning expressions and ordering expressions as follows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from pyspark.sql.window import Window  \n",
    "windowSpec = \\  \n",
    "   Window \\  \n",
    "       .partitionBy(...) \\  \n",
    "       .orderBy(...)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a window:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "w =  Window.partitionBy(DFSpentData.Name).orderBy(DFSpentData.AmountSpent.desc())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a rule of thumb window definitions should always contain  \n",
    "PARTITION BY clause otherwise Spark will move all data to a single partition.  \n",
    "ORDER BY is required for some functions, while in different cases (typically aggregates) may be optional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import rank\n",
    "\n",
    "DFSpentData.select(\n",
    "    \"Name\", \"Date\",\"Mode\", \"AmountSpent\",\n",
    "    rank().over(w).alias(\"rankAmount\")\n",
    ").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
