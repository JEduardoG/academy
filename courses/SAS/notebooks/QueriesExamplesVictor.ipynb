{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importar librerias utiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "import sqlContext.implicits._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seleccion de tablas de SINFO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El orígen de datos de Plan Uno es principalmente tablas de sinfo y también alguna tabla \"core\". Estas tablas están persistidas en HDFS en las rutas correspondientes: \"/data/master/sfma/\" para sinfo y \"/data/master/kdo/\" para core. Todas las tablas están persistidas en formato \"parquet\". \n",
    "\n",
    "Para leer una tabla en formato parquet usaremos el contexto de SparkSQL: \n",
    ">sqlContext.read.parquet(\"PATH_HDFS\"). \n",
    "\n",
    "Definimos la variable pathData par reutilizarla a la hora de leer diferentes tablas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val pathData = \"/data/master/sfma/\"\n",
    "val payrolls = sqlContext.read.parquet(pathData + \"t_sfma_payroll_m\")\n",
    "\n",
    "val generalParty = sqlContext.read.parquet(pathData + \"t_sfma_general_party_m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Su equivalente en SAS para la lectura de tablas:\n",
    "\n",
    "proc sql;  \n",
    "create table payrolls as  \n",
    "select *  \n",
    "from sfmapv.vsfmapoa;  \n",
    "quit;\n",
    "\n",
    "proc sql;  \n",
    "create table generalParty as  \n",
    "select *  \n",
    "from sfmapv.vsfmaiii(de todos los periodos);  \n",
    "quit;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Queries típicas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para hacer transformaciones sobre las tablas, utilizaremos en este caso la api de dataframes en scala (para la versión de spark 1.6.2 :\n",
    "https://spark.apache.org/docs/1.6.2/api/scala/index.html#org.apache.spark.sql.DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Seleccionar columnas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val nominas = payrolls.\n",
    "select(\"customer_id\",\"total_credited_1m_amount\",\"closing_date\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PROC SQL;  \n",
    "create table nominas as  \n",
    "select cod_persona, imp_apuzm, fec_cierre  \n",
    "from payrolls;  \n",
    "quit;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminar columnas: \n",
    "\n",
    "\n",
    "Nota: En la versión 1.6.2 no es posible utilizar la sentencia drop para más de una columna, por lo que tendremos que utilizar un drop por cada columna a eliminar. Sin embargo, en la versión de Spark 2.1 (próxima versión de la plataforma) se podran eliminar más de una columna por sentencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val nominasSinMes = nominas.\n",
    "drop(\"closing_date\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data nominasSinMes;  \n",
    "set nominas(drop=fec_cierre);  \n",
    "run;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtrar registros de una tabla según una condición:\n",
    "\n",
    "Filtro de clientes titulares de algun contrato por mes desde el mes de enero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val intervininentesDesdeEnero = generalParty.\n",
    "select(\"customer_id\",\"closing_date\").\n",
    "where(\"closing_date >= '2017-01-31' AND party_type_id = 'TIT'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PROC SQL;  \n",
    "create table intervininentesEnero as  \n",
    "select cod_persona, fec_cierre  \n",
    "from generalParty  \n",
    "where cod_mes >= 1 and cod_ano eq 2017 and cod_interv='TIT';  \n",
    "quit;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quitar los duplicados:\n",
    "\n",
    "Eliminar duplicados ya que aparecera un registro por cada contrato que tuviese el cliente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val intervininentesEneroSinDup = intervininentesEnero.\n",
    "dropDuplicates(Seq(\"customer_id\",\"closing_date\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "proc sort data=intervininentesEneroSinDup out=intervininentesEnero nodupkey;  \n",
    "by cod_persona, fec_cierre;  \n",
    "run;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Operaciones de agregación:\n",
    "\n",
    "Sumar cantidades por cliente y mes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val nominaMensualPorCliente = payrolls.\n",
    "select(\"customer_id\",\"total_credited_1m_amount\",\"closing_date\").\n",
    "groupBy(\"customer_id\",\"closing_date\").\n",
    "agg(sum(\"total_credited_1m_amount\").\n",
    "as(\"nomina_mensual\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "proc sql;\n",
    "create table nominaMensualPorCliente as\n",
    "select cod_persona, fec_cierre, sum(imp_apuzm) as nomina_mensual \n",
    "from payrolls\n",
    "group by cod_persona, fec_cierre;\n",
    "quit;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tambien se pueden sumar poniendo restricciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val nominaMensualPorClientePositiva = payrolls.\n",
    "select(\"customer_id\",\"total_credited_1m_amount\",\"closing_date\").\n",
    "groupBy(\"customer_id\",\"closing_date\").\n",
    "agg(sum(\"total_credited_1m_amount\").\n",
    "as(\"nomina_mensual\")).\n",
    "where(\"nomina_mensual>0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "proc sql;  \n",
    "create table nominaMensualPorClientePositiva as  \n",
    "select cod_persona, fec_cierre, sum(imp_apuzm) as nomina_mensual  \n",
    "from payrolls  \n",
    "group by cod_persona, fec_cierre  \n",
    "having nomina_mensual>0;  \n",
    "quit;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cruce de dos tablas:  \n",
    "\n",
    "\n",
    "Cruce por variables de cod_persona y mes para sacar la nomina mensual de los intervinientes titulares de algun contrato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val nominasTitulares = nominaMensualPorClientePositiva.\n",
    "join(intervininentesEneroSinDup, Seq(\"customer_id\",\"closing_date\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data nominasTitulares;  \n",
    "merge nominaMensualPorClientePositiva(in=x)  \n",
    "intervininentesEneroSinDup(in=y);  \n",
    "by cod_persona fec_cierre;  \n",
    "if x and y;  \n",
    "run;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En Spark el join por defecto es inner join, si queremos otro tipo de join: full, left..) es necesario especificarlo. \n",
    "En ocasiones cuando se realiza un full join es necesario sustituir los nulos que se generan a un valor específico.\n",
    "\n",
    "Con na.fill(0) rellenamos con 0s todos los huecos vacios que han quedado de hacer el full join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val nominasTitulares = nominaMensualPorClientePositiva.\n",
    "join(intervininentesEneroSinDup, Seq(\"customer_id\",\"closing_date\"),\"full\").\n",
    "na.fill(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data nominasTitulares(drop=i);  \n",
    "merge nominaMensualPorClientePositiva(in=x)   \n",
    "intervininentesEneroSinDup(in=y);  \n",
    "by cod_persona fec_cierre;  \n",
    "if x or y;  \n",
    "ARRAY cero _numeric_;  \n",
    "DO I=1 TO dim(cero);  \n",
    "IF cero(i)=. THEN cero(i)=0;  \n",
    "END;  \n",
    "run;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Añadir una columna con 1 si la nomina del cliente es mayor que 200€ o 0 si no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val nominasTitulares2 = nominasTitulares.\n",
    "withColumn(\"mayor_a_200\",when($\"nomina_mensual\" > 200,1).\n",
    "           otherwise(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data nominasTitulares2;  \n",
    "set nominasTitulares;  \n",
    "if nomina_mensual>200 then mayor_a_200=1;  \n",
    "else mayor_a_200=0;  \n",
    "run;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Renombrar una columna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val nominasTitulares3 = nominasTitulares2.\n",
    "withColumnRenamed(\"mayor_a_200\",\"cumple_nomina_mayor_200\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data nominasTitulares3;  \n",
    "set nominasTitulares2(rename=(mayor_a_200=cumple_nomina_mayor_200));  \n",
    "run;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ordenar un DataFrame de forma ascendente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nominasTitulares3.\n",
    "orderBy($\"nomina_mensual\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "proc sort data=nominasTitulares3;  \n",
    "by nomina_mensual;  \n",
    "run;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ordenar un DataFrame de forma descendente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nominasTitulares3.\n",
    "orderBy($\"nomina_mensual\".desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "proc sort data=nominasTitulares3;  \n",
    "by descending nomina_mensual;  \n",
    "run;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es importante saber que en Spark no es necesario ordenar los DataFrame para hacer join. Simplemente nos serviria para quitar duplicados o visualizar los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcular el número de registros de un dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nominasTitulares.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nominasTitulares.where(\"closing_date='2017-01-31'\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mostrar registros del dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nominasTitulares.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejemplo de mostrar un conteo agrupado y ordenado. Numero de clientes por segmento global en la CGT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sqlContext.read.parquet(pathData + \"t_sfma_ownership_h_category_ah_m\").\n",
    "where(closing_date='2017-01-31').\n",
    "select(\"global_segment_id\",\"customer_id\").\n",
    "groupBy(\"global_segment_id\").\n",
    "count().\n",
    "withColumnRenamed(\"count()\",\"numero_clientes\")\n",
    "orderBy($\"numero_clientes\").desc).\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ver la estructura de la tabla, sus columnas y que formato tienen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nominasTitulares.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para hacer operaciones que implican diferentes columnas de una fila se utilizan las User Defined Functions (UDFs):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.udf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un caso típico es generar una columna nueva cuyo valor depende del valor de una o varias columnas: \n",
    "\n",
    "Ejemplo: Con varios case para, por ejemplo, añadir una columna con la descripcion del tipo de seguro que equivale a ciertos codigos de producto de la tabla de intervinientes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val comCategoryUdf = udf((com_category_id: Long) => {\n",
    "    com_category_id match {\n",
    "        case t if t == 592 => \"HOGAR/INCENDIOS\"\n",
    "        case t if t == 593 => \"NEGOCIOS\"\n",
    "        case t if t == 594 || t== 595 => \"VIDA\"\n",
    "        case t if t == 596 => \"SALUD\"\n",
    "        case t if t == 598 => \"AUTO\"\n",
    "        case _ => \"OTROS\"\n",
    "    }})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val intervinientesSeguros = sqlContext.read.parquet(pathData + \"t_sfma_general_party_m\").\n",
    "where(\"closing_date = '2017-01-31' AND party_type_id='TIT' AND contract_status_id = 'A'\").\n",
    "select(\"customer_id\", \"contract_id\", \"com_category_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "proc sql;  \n",
    "create table intervinientesSeguros as  \n",
    "select cod_persona, cod_idcontra, cod_ctgcom  \n",
    "from sfmapv.vsfmaiii004 (periodo que equivalga a enero)  \n",
    "where cod_interv='TIT' and xti_estado='A';  \n",
    "quit;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para utilizar la función udf en una query bastará con nombrarla y pasarle como parámetro la/s columnas que utiliza:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val intervinientesSegurosNombre = intervinientesSeguros.\n",
    "withColumn(\"tipo_seguro\",comCategoryUdf($\"com_category_id\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data intervinientesSegurosNombre;  \n",
    "set intervinientesSeguros;  \n",
    "if cod_ctgcom=592 then tipo_seguro = 'HOGAR/INCENDIOS';  \n",
    "else if cod_ctgcom=593 then tipo_seguro = 'NEGOCIOS';  \n",
    "else if cod_ctgcom=594 or cod_ctgcom=595 then tipo_seguro = 'VIDA';  \n",
    "else if cod_ctgcom=596 then tipo_seguro = 'SALUD';  \n",
    "else if cod_ctgcom=598 then tipo_seguro = 'AUTO';  \n",
    "else tipo_seguro='OTROS';  \n",
    "run;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el caso de que la operación a realizar es cambiar los valores de una columna según un diccionario, como por ejemplo cmabiar el codigo de segmento por su descripción, crearemos un HashMap para definir la relación de cada código con su descripción, a continuación definiremos una udf que haga uso de ese HashMap:\n",
    "Nota: se puede definir un valor por defecto del HashMap, en este caso será \"PAR_ESTÁNDAR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val globalSegmentDescription = new HashMap[String,String](){override def default(key:String) = \"PAR_ESTÁNDAR\"}\n",
    "\n",
    "globalSegmentDescription +=(\n",
    "    \"1\" -> \"INSTITUCIONES PRIVADAS\",\n",
    "    \"2\" -> \"I.PÚBLICAS SEGURIDAD SOCIAL\",\n",
    "    \"3\" -> \"I.PÚBLICAS ADMON.CENTRAL\",\n",
    "    \"4\" -> \"I.PÚBLICAS CCAA\",\n",
    "    \"5\" -> \"I.PÚB C.LOC – DIP CABILD CONS\",\n",
    "    \"6\" -> \"I.PÚB C.LOC – AYTOS 500k HAB\",\n",
    "    \"7\" -> \"I.PÚB C.LOCALES – AYTOS 20K H\",\n",
    "    \"8\" -> \"I.PÚB C.LOC – AYTOS 5K HAB\",\n",
    "    \"9\" -> \"I.PÚB C.LOC – AYTOS < 5K HAB\",\n",
    "    \"11\" -> \"BPRI_PATRIMONIOS\",\n",
    "    \"12\" -> \"BPRI_BANCA PRIVADA\",\n",
    "    \"13\" -> \"BPER_ALTO VALOR\",\n",
    "    \"14\" -> \"BPER_VALOR RECURSOS\",\n",
    "    \"15\" -> \"BPER_VALOR TENENCIA\", \n",
    "    \"21\" -> \"PAR_POTENCIAL VALOR\",\n",
    "    \"22\" -> \"PAR_POTENCIAL RESTO\",\n",
    "    \"23\" -> \"PAR_59+\",\n",
    "    \"24\" -> \"PAR_ESTÁNDAR\",\n",
    "    \"25\" -> \"PAR_JÓVENES\",\n",
    "    \"31\" -> \"BEC_BANCA CORPORATIVA\",\n",
    "    \"32\" -> \"RE_FINANCIACIÓN RESIDENCIAL\",\n",
    "    \"33\" -> \"BEC_GRANDES EMPRESAS\",\n",
    "    \"34\" -> \"BEC_EMPRESAS\",\n",
    "    \"35\" -> \"PYM_PEQUEÑAS EMPRESAS\",\n",
    "    \"38\" -> \"RE_FINANCIACIÓN PATRIMONIAL\",\n",
    "    \"63\" -> \"RESTO\",\n",
    "    \"80\" -> \"PYM_AUTÓNOMOS Y PROFS VALOR\",\n",
    "    \"81\" -> \"PYM_AUTÓNOMOS Y PROFS RESTO\",\n",
    "    \"82\" -> \"PYM_COMERCIOS VALOR\",\n",
    "    \"83\" -> \"PYM_COMERCIOS RESTO\",\n",
    "    \"84\" -> \"PYM_AGRARIOS VALOR\",\n",
    "    \"85\" -> \"PYM_AGRARIOS RESTO\",\n",
    "    \"86\" -> \"PYM_INDUSTRIA VALOR\",\n",
    "    \"87\" -> \"PYM_INDUSTRIA RESTO\",\n",
    "    \"88\" -> \"PYM_OTRAS PYMES VALOR\",\n",
    "    \"89\" -> \"PYM_OTRAS PYMES RESTO\")\n",
    "    \n",
    "val udfglobalSegmentDescription = udf((segmentCode: String)=> {\n",
    "        globalSegmentDescription(segmentCode)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También se podría definir una udf con sentencias if-else, sin embargo es más lento de ejecutar dado que tiene que ir recorriendo las diferentes sentencias hasta que se cumple la sentencia correcta:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val globalSegmentDescription = udf((segmentCode: String)=> {\n",
    "    if (segmentCode == \"1\") \"INSTITUCIONES PRIVADAS\"\n",
    "    else if (segmentCode == \"2\") \"I.PÚBLICAS SEGURIDAD SOCIAL\"\n",
    "    else if (segmentCode == \"3\") \"I.PÚBLICAS ADMON.CENTRAL\"\n",
    "    else if (segmentCode == \"4\") \"I.PÚBLICAS CCAA\"\n",
    "    else if (segmentCode == \"5\") \"I.PÚB C.LOC – DIP CABILD CONS\"\n",
    "    else if (segmentCode == \"6\") \"I.PÚB C.LOC – AYTOS 500k HAB\"\n",
    "    else if (segmentCode == \"7\") \"I.PÚB C.LOCALES – AYTOS 20K H\"\n",
    "    else if (segmentCode == \"8\") \"I.PÚB C.LOC – AYTOS 5K HAB\"\n",
    "    else if (segmentCode == \"9\") \"I.PÚB C.LOC – AYTOS < 5K HAB\"\n",
    "    else if (segmentCode == \"11\") \"BPRI_PATRIMONIOS\"\n",
    "    else if (segmentCode == \"12\") \"BPRI_BANCA PRIVADA\"\n",
    "    else if (segmentCode == \"13\") \"BPER_ALTO VALOR\"\n",
    "    else if (segmentCode == \"14\") \"BPER_VALOR RECURSOS\"\n",
    "    else if (segmentCode == \"15\") \"BPER_VALOR TENENCIA\"\n",
    "    else if (segmentCode == \"21\") \"PAR_POTENCIAL VALOR\"\n",
    "    else if (segmentCode == \"22\") \"PAR_POTENCIAL RESTO\"\n",
    "    else if (segmentCode == \"23\") \"PAR_59+\"\n",
    "    else if (segmentCode == \"24\") \"PAR_ESTÁNDAR\"\n",
    "    else if (segmentCode == \"25\") \"PAR_JÓVENES\"\n",
    "    else if (segmentCode == \"31\") \"BEC_BANCA CORPORATIVA\"\n",
    "    else if (segmentCode == \"32\") \"RE_FINANCIACIÓN RESIDENCIAL\"\n",
    "    else if (segmentCode == \"33\") \"BEC_GRANDES EMPRESAS\"\n",
    "    else if (segmentCode == \"34\") \"BEC_EMPRESAS\"\n",
    "    else if (segmentCode == \"35\") \"PYM_PEQUEÑAS EMPRESAS\"\n",
    "    else if (segmentCode == \"38\") \"RE_FINANCIACIÓN PATRIMONIAL\"\n",
    "    else if (segmentCode == \"63\") \"RESTO\"\n",
    "    else if (segmentCode == \"80\") \"PYM_AUTÓNOMOS Y PROFS VALOR\"\n",
    "    else if (segmentCode == \"81\") \"PYM_AUTÓNOMOS Y PROFS RESTO\"\n",
    "    else if (segmentCode == \"82\") \"PYM_COMERCIOS VALOR\"\n",
    "    else if (segmentCode == \"83\") \"PYM_COMERCIOS RESTO\"\n",
    "    else if (segmentCode == \"84\") \"PYM_AGRARIOS VALOR\"\n",
    "    else if (segmentCode == \"85\") \"PYM_AGRARIOS RESTO\"\n",
    "    else if (segmentCode == \"86\") \"PYM_INDUSTRIA VALOR\"\n",
    "    else if (segmentCode == \"87\") \"PYM_INDUSTRIA RESTO\"\n",
    "    else if (segmentCode == \"88\") \"PYM_OTRAS PYMES VALOR\"\n",
    "    else if (segmentCode == \"89\") \"PYM_OTRAS PYMES RESTO\"\n",
    "    else \"\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usamos la cgt con datos de enero y seleccionamos el codigo de persona y el codigo de segmento global, para pegarle el significado de ese codigo creamos una nueva columna \"global_segment_type\" con el Udf globalSegmentDescription y la columna de entrada $\"global_segment_id\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val planUnoCustomersBase = sqlContext.read.parquet(pathData + \"t_sfma_ownership_h_category_ah_m\").\n",
    "where(closing_date='2017-01-31').\n",
    "select(\"global_segment_id\",\"customer_id\").\n",
    "withColumn(\"global_segment_type\", globalSegmentDescription($\"global_segment_id\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para buscar el maximo entre columnas de la misma fila necesitamos tambien haremos uso de una UDF, que definaremos previamente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val maxColumnsUdf = udf((x: Double, y: Double, z: Double) => {val a = Array(x,y,z) ; a.reduceLeft(_ max _)})\n",
    "val tabla = tablaEntrada.\n",
    "withColumn(\"maximo\", maxColumnsUdf($\"columna1\",$\"columna2\",$\"columna3\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data tabla;  \n",
    "set tablaEntrada;  \n",
    "maximo=max(columna1,columna2,columna3);  \n",
    "run;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algoritmo para seleccion de los meses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este algoritmo crea una lista con un numero del 1 a la longitud que queramos darle (las veces que se repita el bucle, en este ejemplo 6) que equivale al ultimo dia de cada mes hacia atras. Si estamos en el mes Mayo, 31 de Mayo sera 1, 30 de Abril 2, 31 de Marzo 3, 28 de Febrero 4, etc.\n",
    "Si queremos que el mes anterior sea el 1, en la linea \"calendar.add(Calendar.MONTH, -(i))\" habria que cambiar i por i+1.\n",
    "Para ello creamos un map, donde para el valor i le asignaremos una fecha, en este caso el último día de mes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import java.util.Calendar\n",
    "import java.sql.Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "var lastDateMonth = collection.mutable.Map[Int, Date]()\n",
    "for(i <- 1 to 6){\n",
    "val calendar = Calendar.getInstance()\n",
    "calendar.add(Calendar.MONTH, -(i))\n",
    "val max = calendar.getActualMaximum(Calendar.DAY_OF_MONTH)\n",
    "calendar.set(Calendar.DAY_OF_MONTH, max)\n",
    "lastDateMonth += (i-> new Date(calendar.getTimeInMillis()))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definicion de variables para filtros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada vez que creas una variable tienes que poner val seguido del nombre. Esto seria algo como las macrovariables de SAS, ademas hay que hacerlo al crear tablas, textos, etc. Aqui se crean el codigo de banco 0182 en formato string que equivaldria al filtro necesario para cod_entalfa y el codigo de area de negocio de Consumer Finance para poder usarlos como filtros luego.\n",
    "\n",
    "Ademas definimos las variables que contienen los códigos de pensiones y desempleo y la cantidad minima para cumplir el criterio de pension y desempleo en Plan UNO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val bbvaBankCode = \"0182\"\n",
    "val consumerFinanceBusinessArea = 6057\n",
    "\n",
    "val pensionGroupingId = (72002, 72003)\n",
    "val unemploymentGroupingId =  72004\n",
    "val minPension = 540\n",
    "val minUnemployment = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplo tipico, criterios de pension y desempleo de Plan UNO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para realizar cualquier transformación (select,filter, group...) de un dataframe, se aplicará un método sobre el dataframe, cada transformación va precedida por un \".\"-\n",
    "Aqui creamos las tablas payrolls6m, que seria una tabla que coge informacion de t_sfma_payroll_m (la POA) de los últimos 6 meses (${lastDateMonth(6)}) con código de banco 0182 y área de negocio diferente de Consumer Finance. Además seleccionamos las columnas que vamos a usar despues.\n",
    "\n",
    "Para incluir variables definidas previamente en un string (como la fecha, o los filtros de banco y area de negocio) tenemos que incluir una s previo al string que define la query, a continuación vemos el ejemplo en la sentencia where, las variables las indicaremos dentro de la sentenci precedidas por el símbolo $. Si además la variable de entrada es de tipo string la tenemos que meter dentro de ''."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val payrolls6m = sqlContext.read.parquet(\"/data/master/sfma/t_sfma_payroll_m\").\n",
    "where(s\"closing_date >= '${lastDateMonth(6)}' AND entity_id= $bbvaBankCode AND business_area_id!= $consumerFinanceBusinessArea\").\n",
    "select(\"customer_id\",\"total_credited_1m_amount\",\"closing_date\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%macro meses(periodo);  \n",
    "proc sql;  \n",
    "create table payrolls&periodo.m as  \n",
    "select cod_persona, imp_apuzm, fec_cierre  \n",
    "from sfmapv.vsfmaiii0&periodo  \n",
    "where cod_entalfa eq '0182' and cod_areanego ne 6057;  \n",
    "quit;  \n",
    "%mend;  \n",
    "%meses(01);  \n",
    "%meses(02);  \n",
    "%meses(03);  \n",
    "%meses(04);  \n",
    "%meses(05);  \n",
    "%meses(06);  \n",
    "\n",
    "data payrolls6m;  \n",
    "set payrolls01m payrolls02m payrolls03m payrolls04m payrolls05m payrolls06m;  \n",
    "run;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos las tablas pension y unemployment añadiendo los filtros de la agregacion comercial (com_grouping_id). Luego sumamos la cantidad percibida por cliente al mes y eliminamos los clientes que no tengan ingresos >0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val pension = payrolls6m.\n",
    "where(s\"com_grouping_id in $pensionGroupingId\").\n",
    "select(\"customer_id\",\"total_credited_1m_amount\",\"closing_date\").\n",
    "groupBy(\"customer_id\",\"closing_date\").\n",
    "agg(sum(\"total_credited_1m_amount\").\n",
    "as(\"pension_amount_m1\")).\n",
    "where(\"pension_amount_m1>0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "proc sql;  \n",
    "create table pension as  \n",
    "select cod_persona, fec_cierre, sum(imp_apuzm) as pension_amount_m1  \n",
    "from payrolls6m  \n",
    "group by cod_persona, fec_cierre  \n",
    "having pension_amount_m1>0;  \n",
    "quit;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val unemployment = payrolls6m.\n",
    "where(s\"com_grouping_id = $unemploymentGroupingId\").\n",
    "select(\"customer_id\",\"total_credited_1m_amount\",\"closing_date\").\n",
    "groupBy(\"customer_id\",\"closing_date\").\n",
    "agg(sum(\"total_credited_1m_amount\").\n",
    "as(\"unemployment_amount_m1\")).\n",
    "where(\"unemployment_amount_m1>0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "proc sql;  \n",
    "create table unemployment as  \n",
    "select cod_persona, fec_cierre, sum(imp_apuzm) as unemployment_amount_m1  \n",
    "from payrolls6m  \n",
    "group by cod_persona, fec_cierre  \n",
    "having unemployment_amount_m1>0;  \n",
    "quit;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cruzamos ambas tablas para crear una con un full join (necesitamos los clientes de ambas tablas), rellenamos con 0s los ingresos vacios de clientes que no tengan ambos ingresos de pension y desempleo. Despues creamos un indicador que nos va a mostrar con 1 los clientes que tienen ingresos de pension o desempleo mayores que los minimos necesarios para cumplir el criterio o 0 si no lo cumplen. El resultado seria una tabla por cliente y mes con los indicadores y los ingresos en €."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val definitivePensionUnemploymentSchema = pension.\n",
    "join(unemployment,Seq(\"customer_id\",\"closing_date\"),\"full\").\n",
    "na.fill(0).\n",
    "withColumn(\"pension_type\",when($\"pension_amount_m1\">minPension,1).otherwise(0)).\n",
    "withColumn(\"unemployment_type\",when($\"unemployment_amount_m1\">minUnemployment,1).otherwise(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data definitivePensionUnemploymentSchema(drop=i);  \n",
    "merge pension(in=x)  \n",
    "unemployment(in=y);  \n",
    "by cod_persona fec_cierre;  \n",
    "if x or y;  \n",
    "ARRAY cero numeric;  \n",
    "DO I=1 TO dim(cero);  \n",
    "IF cero(i)=. THEN cero(i)=0;  \n",
    "END;  \n",
    "if pension_amount_m1>300 then pension_type=1; else pension_type=0;  \n",
    "if unemployment_amount_m1>300 then unemployment_type=1; else unemployment_type=0;  \n",
    "run;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Spark se ejecuta de forma \"lazy\", esto quiero decir, que hasta que no se realiza una accion sobre un dataframe no se \"ejecutan\" las transformaciones que le preceden. Acciones son por ejemplo count(), show(), take(), etc. Cuando lancemos una query con una de estas operaciones se ejecutarán todas las transformaciones anteriores. Es decir, cuando lanzamos una celda que incluye una acción, en realidad se ejecutan todas las transformaciones que estén involucradas para realizar dicha accion. Las operaciones más costosas en un sistema distribuido como es Spark, son aquellas que requieren movimiento de datos entre máquinas, es lo que se denomina \"shuffle\", estas operaciones incluyen, groupBy, sort, dropDuplicates, join...etc. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "file_extension": ".scala",
   "name": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
