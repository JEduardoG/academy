{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<img src=\"../static/logo.png\" alt=\"datio\" style=\"width: 200px \"align=\"right\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframe Python vs Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Packages\n",
    "\n",
    "Let's first import all the packages that you will need. \n",
    "\n",
    "- [pandas](http://pandas.pydata.org/)\n",
    "- [pyspark](http://spark.apache.org/docs/2.1.0/api/python/pyspark.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyspark\n",
    "\n",
    "from pyspark.sql.context import SQLContext\n",
    "sc = pyspark.SparkContext('local[*]')\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Reading csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SAS**:\n",
    "\n",
    "SAS **proc import** is usually a good starting point for reading a delimited ASCII data file, such as a .csv (comma-separated values) file or a tab-delimited file.\n",
    "\n",
    "*proc import datafile=\"DATA.csv\" out=mydata dbms=dlm replace; delimiter=\",\"; getnames=yes;run*;\n",
    "\n",
    "**PYTHON**:\n",
    "\n",
    "With Pandas, you easily read CSV files with **read_csv(path_file)**. \n",
    "\n",
    "**SPARK**:\n",
    "\n",
    "Spark DataFrame supports reading data from popular professional formats, like JSON files, Parquet files, Hive table — be it from local file systems, distributed file systems (HDFS), cloud storage (S3), or external relational database systems. But CSV is not supported natively by Spark. You have to use a separate library: spark-csv. \n",
    "Both pandas and Spark Dataframes can easily read multiple formats including CSV, JSON, and some binary formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataPath = \"../data/ttgofici.csv\"\n",
    "\n",
    "#PYTHON\n",
    "pandasDF = pd.read_csv(dataPath)\n",
    "#SPARK\n",
    "sparkDF = sqlContext.read.format('com.databricks.spark.csv').options(header='true').load(dataPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## PYTHON : Count non NA/null observations of each column\n",
    "pandasDF.count()\n",
    "len(pandasDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# SPARK : Count number of rows\n",
    "sparkDF.count() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Viewing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## PYTHON\n",
    "pandasDF.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## SPARK\n",
    "sparkDF.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## SPARK\n",
    "sparkDF.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Inferring Types "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## PYTHON\n",
    "pandasDF.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 - Cast the values in a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#PYTHON:\n",
    "# also : pandasDF['f_cierre'] = pandasDF['f_cierre'].astype('datetime64[ns]')\n",
    "pandasDF['f_cierre'] = pd.to_datetime(pandasDF['f_cierre'])\n",
    "pandasDF.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#SPARK:\n",
    "# With Spark DataFrames loaded from CSV files, default types are assumed to be “strings”. \n",
    "sparkDF.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sparkDF = sqlContext.read.format('com.databricks.spark.csv')\\\n",
    ".options(header='true')\\\n",
    ".option(\"inferSchema\", \"true\")\\\n",
    ".load(dataPath)\n",
    "sparkDF.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#  SPARK: Change types of columns\n",
    "from pyspark.sql.types import DateType\n",
    "sparkDF = sparkDF.withColumn(\"f_cierre\", sparkDF.f_cierre.cast(DateType()))\n",
    "sparkDF.select(\"f_cierre\").schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 - Reading and apply customized schema with Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "    \n",
    "customSchema = StructType([\n",
    " StructField(\"cod_bancsb\",  StringType(), True),\n",
    " StructField(\"cod_ofici\",  IntegerType(), True),\n",
    " StructField(\"cnivel\",  StringType(), True),\n",
    " StructField(\"cod_zona\",  StringType(), True),\n",
    " StructField(\"cod_territor\",  StringType(), True),\n",
    " StructField(\"cod_dirgener\",  StringType(), True),\n",
    " StructField(\"cod_areanego\",  IntegerType(), True),\n",
    " StructField(\"cod_dar\",  StringType(), True),\n",
    " StructField(\"des_nomco\",  StringType(), True),\n",
    " StructField(\"des_nomab\",  StringType(), True),\n",
    " StructField(\"f_cierre\",  StringType(), True),\n",
    " StructField(\"cod_cbc\",  DateType(), True)])\n",
    "\n",
    "sparkDFSchemaApplied = sqlContext.read.format(\"com.databricks.spark.csv\")\\\n",
    "            .option(\"header\", \"true\")\\\n",
    "            .load(dataPath, schema=customSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sparkDFSchemaApplied.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8 - Describing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Pandas and Spark, .describe() generate various summary statistics. They could give slightly different results for two reasons: \n",
    "\n",
    "\n",
    "1) In Pandas, NaN values are excluded. In Spark, NaN values make that computation of mean and standard deviation fail\n",
    "\n",
    "2) standard deviation is not computed in the same way. Unbiased (or corrected) standard deviation by default in Pandas, and uncorrected standard deviation in Spark. The difference is the use of N-1 instead of N on the denominator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#PYTHON:\n",
    "pandasDF.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#SPARK:\n",
    "sparkDF.describe().show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
