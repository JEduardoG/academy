{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<img src=\"../static/logo.png\" alt=\"datio\" style=\"width: 200px \"align=\"right\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframe Python vs Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#PYTHON\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#SPARK\n",
    "import pyspark\n",
    "from pyspark.sql.context import SQLContext\n",
    "sc = pyspark.SparkContext('local[*]')\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SAS: SAS proc import is usually a good starting point for reading a delimited ASCII data file, such as a .csv (comma-separated values) file or a tab-delimited file. Sometimes we can also use a data step to read in an ASCII data file. On this page, we will show examples on how to read delimited ASCII files using proc import and data step.\n",
    "\n",
    "proc import datafile=\"DATA.csv\" out=mydata dbms=dlm replace;\n",
    "\n",
    "   delimiter=\",\";\n",
    "   \n",
    "   getnames=yes;\n",
    "   \n",
    "run;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PYTHON : With Pandas, you easily read CSV files with read_csv(). \n",
    "\n",
    "SPARK: Spark DataFrame supports reading data from popular professional formats, like JSON files, Parquet files, Hive table — be it from local file systems, distributed file systems (HDFS), cloud storage (S3), or external relational database systems. But CSV is not supported natively by Spark. You have to use a separate library: spark-csv. \n",
    "Both pandas and Spark Dataframes can easily read multiple formats including CSV, JSON, and some binary formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataPath = \"../data/ttgofici.csv\"\n",
    "#PYTHON\n",
    "pandasDF = pd.read_csv(dataPath)\n",
    "#SAS\n",
    "sparkDF = sqlContext.read.format('com.databricks.spark.csv').options(header='true').load(dataPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Count non NA/null observations of each column\n",
    "pandasDF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Number of rows\n",
    "sparkDF.count() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viewing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pandasDF.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sparkDF.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sparkDF.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferring Types "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pandasDF.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#PYTHON: CAST THE VALUES IN A COLUMN\n",
    "# also : pandasDF['f_cierre'] = pandasDF['f_cierre'].astype('datetime64[ns]')\n",
    "pandasDF['f_cierre'] = pd.to_datetime(pandasDF['f_cierre'])\n",
    "pandasDF.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# With Spark DataFrames loaded from CSV files, default types are assumed to be “strings”. \n",
    "sparkDF.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sparkDF = sqlContext.read.format('com.databricks.spark.csv')\\\n",
    ".options(header='true')\\\n",
    ".option(\"inferSchema\", \"true\")\\\n",
    ".load(dataPath)\n",
    "sparkDF.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#  SPARK: Change types of columns\n",
    "from pyspark.sql.types import DateType\n",
    "sparkDF = sparkDF.withColumn(\"f_cierre\", sparkDF.f_cierre.cast(DateType()))\n",
    "sparkDF.select(\"f_cierre\").schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and apply customized schema with Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "    \n",
    "customSchema = StructType([\n",
    " StructField(\"cod_bancsb\",  StringType(), True),\n",
    " StructField(\"cod_ofici\",  IntegerType(), True),\n",
    " StructField(\"cnivel\",  StringType(), True),\n",
    " StructField(\"cod_zona\",  StringType(), True),\n",
    " StructField(\"cod_territor\",  StringType(), True),\n",
    " StructField(\"cod_dirgener\",  StringType(), True),\n",
    " StructField(\"cod_areanego\",  IntegerType(), True),\n",
    " StructField(\"cod_dar\",  StringType(), True),\n",
    " StructField(\"des_nomco\",  StringType(), True),\n",
    " StructField(\"des_nomab\",  StringType(), True),\n",
    " StructField(\"f_cierre\",  StringType(), True),\n",
    " StructField(\"cod_cbc\",  StringType(), True)])\n",
    "\n",
    "sparkDFSchemaApplied = sqlContext.read.format(\"com.databricks.spark.csv\")\\\n",
    "            .option(\"header\", \"true\")\\\n",
    "            .load(dataPath, schema=customSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#process schema doesn't work with StructType\n",
    "sparkDFSchemaApplied.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Describing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Pandas and Spark, .describe() generate various summary statistics. They could give slightly different results for two reasons: \n",
    "\n",
    "\n",
    "1) In Pandas, NaN values are excluded. In Spark, NaN values make that computation of mean and standard deviation fail\n",
    "\n",
    "2) standard deviation is not computed in the same way. Unbiased (or corrected) standard deviation by default in Pandas, and uncorrected standard deviation in Spark. The difference is the use of N-1 instead of N on the denominator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pandasDF.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sparkDF.describe().show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
